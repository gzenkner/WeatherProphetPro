{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import requests\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "# nvcc --version  ###CUDA version\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "from numpy.random import seed\n",
    "seed(2)\n",
    "tf.random.set_seed(2)\n",
    "from math import sqrt\n",
    "import os\n",
    "\n",
    "random_state = 42\n",
    "# import mlflow\n",
    "# exp_id = 'weather_dataset'\n",
    "# mlflow.set_experiment(exp_id)\n",
    "# mlflow.set_tracking_uri('file:///C:/Users/gabri/VSCode%20Projects/Weather%20Prediction/mlruns')\n",
    "# mlflow.autolog()\n",
    "# mlflow.log_param('random_state', random_state)\n",
    "# export MLFLOW_TRACKING_URI=http://192.168.0.1:5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) define training, testing and prediction daterange \n",
    "# 2) define parameters, feature engineering and transformations, create dataframe \n",
    "# 3) convert to numpy and reshape \n",
    "# 4) define train, test and val split\n",
    "# 5) normalize\n",
    "# 6) define model and log in MLFlow\n",
    "# 7) model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify project dataset and table\n",
    "client = bigquery.Client()\n",
    "project_id = os.environ.get('GOOGLE_CLOUD_PROJECT')\n",
    "dataset_id = 'weather_api'\n",
    "table_id = 'union_weather_api_training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected number of hours between the two dates is: 9023 hours\n",
      "Locations:  clerkenwell hadley_wood\n"
     ]
    }
   ],
   "source": [
    "# specify date range\n",
    "training_start = '2021-08-01 00:00:00'  # Specify the start time with hours, minutes, and seconds\n",
    "training_end = '2022-08-11 23:01:00'  # Specify the end time with hours, minutes, and seconds\n",
    "hours_difference = (datetime.datetime.strptime(training_end, '%Y-%m-%d %H:%M:%S') - datetime.datetime.strptime(training_start, '%Y-%m-%d %H:%M:%S')).total_seconds() / 3600\n",
    "print(f'The expected number of hours between the two dates is: {round(hours_difference)} hours')\n",
    "\n",
    "date_obj_start = datetime.datetime.strptime(training_start, '%Y-%m-%d %H:%M:%S')  # Include '%H:%M:%S'\n",
    "date_obj_end = datetime.datetime.strptime(training_end, '%Y-%m-%d %H:%M:%S')  # Include '%H:%M:%S'\n",
    "unix_start = int(date_obj_start.timestamp())\n",
    "unix_end = int(date_obj_end.timestamp())\n",
    "\n",
    "# specify locations\n",
    "location_1 = 'clerkenwell'\n",
    "location_2 = 'hadley_wood'\n",
    "print('Locations: ', location_1, location_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gets data from 2021-08-01 00:00:00 to 2022-08-11 23:01:00 from clerkenwell and hadley_wood and merges the two tables\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    dt,\n",
    "    city_name,\n",
    "    temp_c\n",
    "FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "where dt > {unix_start} and dt < {unix_end}\n",
    "order by dt\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "df = query_job.to_dataframe()\n",
    "print(f'Gets data from {training_start} to {training_end} from {location_1} and {location_2} and merges the two tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp_clerkenwell</th>\n",
       "      <th>temp_hadley_wood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1627776000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1627779600</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  temp_clerkenwell  temp_hadley_wood\n",
       "0  1627776000              16.0              15.0\n",
       "1  1627779600              16.0              15.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_table = df.pivot(index='dt', columns='city_name', values=['temp_c']).reset_index()\n",
    "pivot_table.columns.name = None  # Remove the name from the columns\n",
    "pivot_table.columns = ['dt', 'temp_clerkenwell', 'temp_hadley_wood']\n",
    "pivot_table.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices with non-3600 differences: []\n",
      "Values of differences at those indices: []\n"
     ]
    }
   ],
   "source": [
    "missing_timestamps = np.diff(pivot_table['dt'])\n",
    "result_indices = np.where(missing_timestamps != 3600)[0]\n",
    "result_values = missing_timestamps[result_indices]\n",
    "print(\"Indices with non-3600 differences:\", result_indices)\n",
    "print(\"Values of differences at those indices:\", result_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['pressure_clerkenwell', 'pressure_hadley_wood'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gabri\\VSCode Projects\\Weather Prediction\\WeatherProphetPro\\train_model.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/VSCode%20Projects/Weather%20Prediction/WeatherProphetPro/train_model.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# create timestamps and data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/VSCode%20Projects/Weather%20Prediction/WeatherProphetPro/train_model.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m timestamps_array \u001b[39m=\u001b[39m pivot_table[\u001b[39m'\u001b[39m\u001b[39mdt\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gabri/VSCode%20Projects/Weather%20Prediction/WeatherProphetPro/train_model.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data_array \u001b[39m=\u001b[39m pivot_table[[\u001b[39m'\u001b[39;49m\u001b[39mtemp_clerkenwell\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtemp_hadley_wood\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpressure_clerkenwell\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpressure_hadley_wood\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39mto_numpy()\n",
      "File \u001b[1;32mc:\\Users\\gabri\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3902\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3900\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3901\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3902\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3904\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gabri\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6116\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6118\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gabri\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6178\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6175\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6177\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 6178\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['pressure_clerkenwell', 'pressure_hadley_wood'] not in index\""
     ]
    }
   ],
   "source": [
    "# create timestamps and data\n",
    "timestamps_array = pivot_table['dt'].to_numpy()\n",
    "data_array = pivot_table[['temp_clerkenwell', 'temp_hadley_wood', 'pressure_clerkenwell', 'pressure_hadley_wood']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset starts on 2021-08-01 00:00:00\n",
      "Dataset ends on (datetime.datetime(2022, 8, 11, 22, 0),)\n",
      "Original length of dataset: 9023 hours\n",
      "Length of context: 24 hours\n",
      "Number of hours to predict : 24 hours\n",
      "Given temperature from the last 24 hours, we want to predict temperature over the next 24 hours\n",
      "In order to create windows, we lose 23 hours to create the first window\n",
      "Number of datapoints for training, validation and testing: 9000 hours\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.datetime.utcfromtimestamp(timestamps_array[0])\n",
    "end_date = datetime.datetime.utcfromtimestamp(timestamps_array[-1]),\n",
    "\n",
    "context_hours = 24\n",
    "prediction_length = 24\n",
    "dataset_length = len(data_array)\n",
    "eliminated_timesteps = context_hours - 1\n",
    "\n",
    "print(f'Dataset starts on {start_date}')\n",
    "print(f'Dataset ends on {end_date}')\n",
    "print(f'Original length of dataset: {dataset_length} hours')\n",
    "\n",
    "\n",
    "print(f'Length of context: {context_hours} hours')\n",
    "print(f'Number of hours to predict : {prediction_length} hours')\n",
    "print(f'Given temperature from the last {context_hours} hours, we want to predict temperature over the next {prediction_length} hours')\n",
    "print(f'In order to create windows, we lose {eliminated_timesteps} hours to create the first window')\n",
    "\n",
    "new_dataset_length = dataset_length - eliminated_timesteps\n",
    "window_count = new_dataset_length\n",
    "print(f'Number of datapoints for training, validation and testing: {new_dataset_length} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of indices available to create window, train, validate and test: 9023\n",
      "Train and validation split: 0.8 - 0.2\n",
      "Eliminated datapoints: 0 - 23\n",
      "Train: 23-7199\n",
      "Validate: 7199-8999\n",
      "Test: 7199 - 9023\n"
     ]
    }
   ],
   "source": [
    "# 7 Define train, val and test ratios\n",
    "print(f'Number of indices available to create window, train, validate and test: {dataset_length}')\n",
    "train = 0.8\n",
    "val = 0.2\n",
    "window_start = 0\n",
    "print(f'Train and validation split: {train} - {val}')\n",
    "print(f'Eliminated datapoints: 0 - {eliminated_timesteps}')\n",
    "train_start = eliminated_timesteps\n",
    "val_start = int((dataset_length-prediction_length)*train)\n",
    "print(f'Train: {train_start}-{val_start}')\n",
    "test_start = dataset_length-prediction_length\n",
    "print(f'Validate: {val_start}-{test_start}')\n",
    "print(f'Test: {val_start} - {dataset_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The behavior of DatetimeProperties.to_pydatetime is deprecated*\")\n",
    "\n",
    "fig = go.Figure()\n",
    "window_trace = px.scatter(x=datetime_list[window_start:train_start], y=final_df['cl_temp'].iloc[window_start:train_start]).data[0]\n",
    "window_trace.update(line=dict(color='purple'), marker=dict(color='purple'), name=f'Eliminated indices: {window_start}-{train_start}. Count:{train_start-window_start}', showlegend=True)\n",
    "\n",
    "# Plot the training data\n",
    "train_trace = px.scatter(x=datetime_list[train_start:val_start], y=final_df['cl_temp'][train_start:val_start]).data[0]\n",
    "train_trace.update(line=dict(color='blue'), marker=dict(color='blue'), name=f'Train Data: {train_start}-{val_start}. Count:{val_start-train_start}', showlegend=True)\n",
    "\n",
    "# Plot the validation data\n",
    "val_trace = px.scatter(x=datetime_list[val_start:test_start], y=final_df['cl_temp'][val_start:test_start]).data[0]\n",
    "val_trace.update(line=dict(color='green'), marker=dict(color='green'), name=f'Validation Data: {val_start}-{test_start}. Count:{test_start-val_start}', showlegend=True)\n",
    "\n",
    "# Plot the test data\n",
    "test_trace = px.scatter(x=datetime_list[test_start: dataset_length], y=final_df['cl_temp'][test_start: dataset_length]).data[0]\n",
    "test_trace.update(line=dict(color='red'), marker=dict(color='red'), name=f'Test Data: {test_start}-{dataset_length}. Count:{dataset_length-test_start}', showlegend=True)\n",
    "\n",
    "# Plot all data\n",
    "all_trace = px.scatter(x=datetime_list, y=final_df['cl_temp'].to_list()).data[0]\n",
    "all_trace.update(line=dict(color='yellow'), marker=dict(color='yellow'), name=f'All Data', showlegend=True, mode='lines')\n",
    "\n",
    "fig.add_trace(window_trace)\n",
    "fig.add_trace(train_trace)\n",
    "fig.add_trace(val_trace)\n",
    "fig.add_trace(test_trace)\n",
    "fig.add_trace(all_trace)\n",
    "\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(final_df.iloc[train_start:val_start])\n",
    "df_scaled = pd.DataFrame(scaler.transform(final_df))\n",
    "df_scaled.columns = final_df.columns\n",
    "print(f'We calculate the mean and std of the training data and use this to normalise the entire dataframe')\n",
    "print(final_df.head(1))\n",
    "print(df_scaled.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = len(df_scaled['cl_temp'])\n",
    "data = df_scaled['cl_temp'].to_numpy()\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(total_length - context_hours):\n",
    "    X.append(data[i:i + context_hours])\n",
    "    y.append(data[i + context_hours:i + context_hours + 1]) \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import mlflow\n",
    "# import numpy as np\n",
    "# from tensorflow import keras\n",
    "# from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, InputLayer\n",
    "\n",
    "# feature_count = len(features)\n",
    "# model = keras.Sequential()\n",
    "\n",
    "# model.add(InputLayer((context_hours, feature_count)))\n",
    "# model.add(Bidirectional(LSTM(64, activation='tanh')))\n",
    "# model.add(Dense(256, 'relu'))\n",
    "# model.add(Dense(64, 'relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(feature_count, 'linear'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.metrics import RootMeanSquaredError\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# learning_rate = 0.0005\n",
    "# epochs = 8\n",
    "# loss = 'mse'\n",
    "# batch_size = 128\n",
    "# model.compile(loss=loss, optimizer=Adam(learning_rate = learning_rate), metrics=[RootMeanSquaredError()])\n",
    "# history = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, batch_size=batch_size, verbose=False)\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scale and get mae\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred = scaler.inverse_transform(y_pred)\n",
    "y_test_final = scaler.inverse_transform(y_test)\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# print(f'MAE: {round(mean_absolute_error(y_test_final, y_pred), 2)}')\n",
    "# print(f'RMSE: {round(sqrt(mean_squared_error(y_test_final, y_pred)), 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(x=[i for i in range(len(y_pred))], y=y_test_final.reshape(len(y_test_final),) )\n",
    "# plt.scatter(x=[i for i in range(len(y_pred))], y=y_pred.reshape(len(y_pred),) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('cf_cl.h5')\n",
    "# model.save('test_model.keras')\n",
    "# # loaded_model = tf.keras.models.load_model('heath.h5')\n",
    "# loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "loaded_model = tf.keras.models.load_model('test_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.save('test_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"test_model_lite\")\n",
    "tflite_model = converter.convert()\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_temperature = X_test[0]\n",
    "true_temperature = y_test_final[0]\n",
    "predicted_temperature_scaled = loaded_model.predict(input_temperature.reshape(1, input_temperature.shape[0]))\n",
    "predicted_temperature = scaler.inverse_transform(predicted_temperature_scaled)\n",
    "\n",
    "print('True temperature: ', true_temperature)\n",
    "print('predicted_temperature: ', predicted_temperature)\n",
    "print('Input data: ', scaler.inverse_transform(input_temperature.reshape(input_temperature.shape[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
