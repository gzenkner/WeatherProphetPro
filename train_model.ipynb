{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime \n",
    "import requests\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "# nvcc --version  ###CUDA version\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "from numpy.random import seed\n",
    "seed(2)\n",
    "tf.random.set_seed(2)\n",
    "from math import sqrt\n",
    "import os\n",
    "\n",
    "random_state = 42\n",
    "# import mlflow\n",
    "# exp_id = 'weather_dataset'\n",
    "# mlflow.set_experiment(exp_id)\n",
    "# mlflow.set_tracking_uri('file:///C:/Users/gabri/VSCode%20Projects/Weather%20Prediction/mlruns')\n",
    "# mlflow.autolog()\n",
    "# mlflow.log_param('random_state', random_state)\n",
    "# export MLFLOW_TRACKING_URI=http://192.168.0.1:5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) define training, testing and prediction daterange \n",
    "# 2) define parameters, feature engineering and transformations, create dataframe \n",
    "# 3) convert to numpy and reshape \n",
    "# 4) define train, test and val split\n",
    "# 5) normalize\n",
    "# 6) define model and log in MLFlow\n",
    "# 7) model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project details and gcp connection\n",
    "project_id = os.environ.get('GOOGLE_CLOUD_PROJECT')\n",
    "key_path = r'C:\\Users\\gabri\\GCP\\cohesive-geode-404308-1242b1e12c66.json'\n",
    "key_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "client = bigquery.Client.from_service_account_json(key_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gets data from 2021-08-01 00:00:00 to 2022-08-11 23:01:00 from City of London and Cockfosters and merges the two tables\n"
     ]
    }
   ],
   "source": [
    "# get data query\n",
    "# training_start = '2021-08-01'\n",
    "# training_end = '2021-08-12'\n",
    "training_start = '2021-08-01 00:00:00'  # Specify the start time with hours, minutes, and seconds\n",
    "training_end = '2022-08-11 23:01:00'  # Specify the end time with hours, minutes, and seconds\n",
    "\n",
    "location_1 = 'City of London'\n",
    "location_2 = 'Cockfosters'\n",
    "\n",
    "# date_obj_start = datetime.datetime.strptime(training_start, '%Y-%m-%d')\n",
    "# date_obj_end = datetime.datetime.strptime(training_end, '%Y-%m-%d')\n",
    "date_obj_start = datetime.datetime.strptime(training_start, '%Y-%m-%d %H:%M:%S')  # Include '%H:%M:%S'\n",
    "date_obj_end = datetime.datetime.strptime(training_end, '%Y-%m-%d %H:%M:%S')  # Include '%H:%M:%S'\n",
    "unix_start = int(date_obj_start.timestamp())\n",
    "unix_end = int(date_obj_end.timestamp())\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "select dt, temp - 273.15 as temp, pressure, humidity\n",
    "from {project_id}.weather_training.weather_city_of_london_training\n",
    "where dt > {unix_start} and dt < {unix_end}\n",
    "order by dt\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "df_filtered_cl = query_job.to_dataframe()\n",
    "\n",
    "query = f\"\"\"\n",
    "select dt, temp - 273.15 as temp, pressure, humidity\n",
    "from {project_id}.weather_training.weather_cockfosters_historical\n",
    "where dt > {unix_start} and dt < {unix_end}\n",
    "order by dt\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "df_filtered_cf = query_job.to_dataframe()\n",
    "print(f'Gets data from {training_start} to {training_end} from {location_1} and {location_2} and merges the two tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cod</th>\n",
       "      <th>name</th>\n",
       "      <th>dt</th>\n",
       "      <th>all</th>\n",
       "      <th>pressure</th>\n",
       "      <th>temp</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>humidity</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>Hadley Wood</td>\n",
       "      <td>1699896297</td>\n",
       "      <td>14</td>\n",
       "      <td>1011</td>\n",
       "      <td>285.55</td>\n",
       "      <td>286.22</td>\n",
       "      <td>284.47</td>\n",
       "      <td>75</td>\n",
       "      <td>284.80</td>\n",
       "      <td>51.6479</td>\n",
       "      <td>-0.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>Hadley Wood</td>\n",
       "      <td>1699898278</td>\n",
       "      <td>15</td>\n",
       "      <td>1012</td>\n",
       "      <td>285.49</td>\n",
       "      <td>286.20</td>\n",
       "      <td>284.47</td>\n",
       "      <td>76</td>\n",
       "      <td>284.76</td>\n",
       "      <td>51.6479</td>\n",
       "      <td>-0.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>Hadley Wood</td>\n",
       "      <td>1699898278</td>\n",
       "      <td>15</td>\n",
       "      <td>1012</td>\n",
       "      <td>285.49</td>\n",
       "      <td>286.20</td>\n",
       "      <td>284.47</td>\n",
       "      <td>76</td>\n",
       "      <td>284.76</td>\n",
       "      <td>51.6479</td>\n",
       "      <td>-0.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>Hadley Wood</td>\n",
       "      <td>1700070556</td>\n",
       "      <td>23</td>\n",
       "      <td>1030</td>\n",
       "      <td>281.15</td>\n",
       "      <td>282.73</td>\n",
       "      <td>279.35</td>\n",
       "      <td>81</td>\n",
       "      <td>281.15</td>\n",
       "      <td>51.6479</td>\n",
       "      <td>-0.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>Hadley Wood</td>\n",
       "      <td>1700070556</td>\n",
       "      <td>23</td>\n",
       "      <td>1030</td>\n",
       "      <td>281.15</td>\n",
       "      <td>282.73</td>\n",
       "      <td>279.35</td>\n",
       "      <td>81</td>\n",
       "      <td>281.15</td>\n",
       "      <td>51.6479</td>\n",
       "      <td>-0.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>200</td>\n",
       "      <td>Hadley Wood</td>\n",
       "      <td>1700208006</td>\n",
       "      <td>5</td>\n",
       "      <td>1031</td>\n",
       "      <td>276.53</td>\n",
       "      <td>278.59</td>\n",
       "      <td>274.35</td>\n",
       "      <td>95</td>\n",
       "      <td>273.50</td>\n",
       "      <td>51.6479</td>\n",
       "      <td>-0.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>200</td>\n",
       "      <td>Hadley Wood</td>\n",
       "      <td>1700215206</td>\n",
       "      <td>6</td>\n",
       "      <td>1032</td>\n",
       "      <td>279.37</td>\n",
       "      <td>281.11</td>\n",
       "      <td>277.79</td>\n",
       "      <td>94</td>\n",
       "      <td>279.37</td>\n",
       "      <td>51.6479</td>\n",
       "      <td>-0.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>200</td>\n",
       "      <td>Hadley Wood</td>\n",
       "      <td>1700218804</td>\n",
       "      <td>6</td>\n",
       "      <td>1033</td>\n",
       "      <td>281.21</td>\n",
       "      <td>283.16</td>\n",
       "      <td>279.18</td>\n",
       "      <td>87</td>\n",
       "      <td>281.21</td>\n",
       "      <td>51.6479</td>\n",
       "      <td>-0.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>200</td>\n",
       "      <td>Hadley Wood</td>\n",
       "      <td>1700222404</td>\n",
       "      <td>8</td>\n",
       "      <td>1033</td>\n",
       "      <td>282.91</td>\n",
       "      <td>284.50</td>\n",
       "      <td>280.67</td>\n",
       "      <td>78</td>\n",
       "      <td>282.91</td>\n",
       "      <td>51.6479</td>\n",
       "      <td>-0.1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>200</td>\n",
       "      <td>Hadley Wood</td>\n",
       "      <td>1700236805</td>\n",
       "      <td>97</td>\n",
       "      <td>1033</td>\n",
       "      <td>282.05</td>\n",
       "      <td>282.89</td>\n",
       "      <td>280.54</td>\n",
       "      <td>86</td>\n",
       "      <td>282.05</td>\n",
       "      <td>51.6479</td>\n",
       "      <td>-0.1513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cod         name          dt  all  pressure    temp  temp_max  temp_min  \\\n",
       "0    200  Hadley Wood  1699896297   14      1011  285.55    286.22    284.47   \n",
       "1    200  Hadley Wood  1699898278   15      1012  285.49    286.20    284.47   \n",
       "2    200  Hadley Wood  1699898278   15      1012  285.49    286.20    284.47   \n",
       "3    200  Hadley Wood  1700070556   23      1030  281.15    282.73    279.35   \n",
       "4    200  Hadley Wood  1700070556   23      1030  281.15    282.73    279.35   \n",
       "..   ...          ...         ...  ...       ...     ...       ...       ...   \n",
       "113  200  Hadley Wood  1700208006    5      1031  276.53    278.59    274.35   \n",
       "114  200  Hadley Wood  1700215206    6      1032  279.37    281.11    277.79   \n",
       "115  200  Hadley Wood  1700218804    6      1033  281.21    283.16    279.18   \n",
       "116  200  Hadley Wood  1700222404    8      1033  282.91    284.50    280.67   \n",
       "117  200  Hadley Wood  1700236805   97      1033  282.05    282.89    280.54   \n",
       "\n",
       "     humidity  feels_like      lat     lon  \n",
       "0          75      284.80  51.6479 -0.1513  \n",
       "1          76      284.76  51.6479 -0.1513  \n",
       "2          76      284.76  51.6479 -0.1513  \n",
       "3          81      281.15  51.6479 -0.1513  \n",
       "4          81      281.15  51.6479 -0.1513  \n",
       "..        ...         ...      ...     ...  \n",
       "113        95      273.50  51.6479 -0.1513  \n",
       "114        94      279.37  51.6479 -0.1513  \n",
       "115        87      281.21  51.6479 -0.1513  \n",
       "116        78      282.91  51.6479 -0.1513  \n",
       "117        86      282.05  51.6479 -0.1513  \n",
       "\n",
       "[118 rows x 12 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "SELECT\n",
    "  cod,\n",
    "  name,\n",
    "  dt,\n",
    "  clouds.*,\n",
    "  main.*,\n",
    "  coord.*\n",
    "FROM `{project_id}.weather_api.weather_api_table`;\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "df = query_job.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to distinguish them by location, each location is treated as a different feature\n",
    "df_filtered_cl.rename(columns={'dt': 'dt_cl', 'temp': 'cl_temp', 'pressure': 'cl_pressure', 'humidity': 'cl_humidity'}, inplace=True)\n",
    "df_filtered_cf.rename(columns={'dt': 'dt_cf', 'temp': 'cf_temp', 'pressure': 'cf_pressure', 'humidity': 'cf_humidity'}, inplace=True)\n",
    "\n",
    "# merge tables / dataframes\n",
    "merged_df = pd.merge(df_filtered_cl, df_filtered_cf, how='left', left_on='dt_cl', right_on='dt_cf')\n",
    "\n",
    "# copy dataframe\n",
    "\n",
    "# filter and copy dataframe columns, select one timestamp and rename \n",
    "columns_to_keep = ['dt_cl', 'cl_temp']\n",
    "features = ['cl_temp']\n",
    "final_df = merged_df[columns_to_keep].copy()\n",
    "final_df.rename(columns={'dt_cl': 'dt'}, inplace=True)\n",
    "\n",
    "print(f\"Timestamp column name changed to 'dt'\")\n",
    "print(f'Features selected: {columns_to_keep}')\n",
    "print(final_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays from dataframe columns, convert Unix timestamp to UTC datetime\n",
    "\n",
    "index_list=[]\n",
    "timestamp_list=[]\n",
    "for idx, timestamp in enumerate(final_df['dt'].to_numpy()):\n",
    "    index_list.append(idx) \n",
    "    timestamp_list.append(timestamp)\n",
    "\n",
    "datetime_list = [datetime.datetime.utcfromtimestamp(event) for event in timestamp_list]\n",
    "final_df = final_df.drop(columns='dt', axis=0)\n",
    "dataset_length = len(index_list)\n",
    "print(f'Number of indices: {dataset_length}')\n",
    "print('Converted dataframe columns to lists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime.utcfromtimestamp(timestamp_list[0])\n",
    "end_date = datetime.datetime.utcfromtimestamp(timestamp_list[-1])\n",
    "\n",
    "context_hours = 24\n",
    "prediction_length = 24\n",
    "eliminated_timesteps = context_hours - 1\n",
    "\n",
    "print(f'Dataset starts on {start_date}')\n",
    "print(f'Dataset ends on {end_date}')\n",
    "print(f'Original length of dataset: {dataset_length} hours')\n",
    "\n",
    "\n",
    "print(f'Length of context: {context_hours} hours')\n",
    "print(f'Number of hours to predict predict: {prediction_length} hours')\n",
    "print(f'Given temperature from the last {context_hours} hours, we want to predict temperature over the next {prediction_length} hours')\n",
    "print(f'In order to create windows, we lose {eliminated_timesteps} hours to create the first window')\n",
    "\n",
    "new_dataset_length = dataset_length - eliminated_timesteps\n",
    "window_count = new_dataset_length\n",
    "print(f'Number of datapoints for training, validation and testing: {new_dataset_length} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Define train, val and test ratios\n",
    "print(f'Number of indices available to create window, train, validate and test: {dataset_length}')\n",
    "train = 0.8\n",
    "val = 0.2\n",
    "window_start = 0\n",
    "print(f'Train and validation split: {train} - {val}')\n",
    "print(f'Eliminated datapoints: 0 - {eliminated_timesteps}')\n",
    "train_start = eliminated_timesteps\n",
    "val_start = int((dataset_length-prediction_length)*train)\n",
    "print(f'Train: {train_start}-{val_start}')\n",
    "test_start = dataset_length-prediction_length\n",
    "print(f'Validate: {val_start}-{test_start}')\n",
    "print(f'Test: {val_start} - {dataset_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The behavior of DatetimeProperties.to_pydatetime is deprecated*\")\n",
    "\n",
    "fig = go.Figure()\n",
    "window_trace = px.scatter(x=datetime_list[window_start:train_start], y=final_df['cl_temp'].iloc[window_start:train_start]).data[0]\n",
    "window_trace.update(line=dict(color='purple'), marker=dict(color='purple'), name=f'Eliminated indices: {window_start}-{train_start}. Count:{train_start-window_start}', showlegend=True)\n",
    "\n",
    "# Plot the training data\n",
    "train_trace = px.scatter(x=datetime_list[train_start:val_start], y=final_df['cl_temp'][train_start:val_start]).data[0]\n",
    "train_trace.update(line=dict(color='blue'), marker=dict(color='blue'), name=f'Train Data: {train_start}-{val_start}. Count:{val_start-train_start}', showlegend=True)\n",
    "\n",
    "# Plot the validation data\n",
    "val_trace = px.scatter(x=datetime_list[val_start:test_start], y=final_df['cl_temp'][val_start:test_start]).data[0]\n",
    "val_trace.update(line=dict(color='green'), marker=dict(color='green'), name=f'Validation Data: {val_start}-{test_start}. Count:{test_start-val_start}', showlegend=True)\n",
    "\n",
    "# Plot the test data\n",
    "test_trace = px.scatter(x=datetime_list[test_start: dataset_length], y=final_df['cl_temp'][test_start: dataset_length]).data[0]\n",
    "test_trace.update(line=dict(color='red'), marker=dict(color='red'), name=f'Test Data: {test_start}-{dataset_length}. Count:{dataset_length-test_start}', showlegend=True)\n",
    "\n",
    "# Plot all data\n",
    "all_trace = px.scatter(x=datetime_list, y=final_df['cl_temp'].to_list()).data[0]\n",
    "all_trace.update(line=dict(color='yellow'), marker=dict(color='yellow'), name=f'All Data', showlegend=True, mode='lines')\n",
    "\n",
    "fig.add_trace(window_trace)\n",
    "fig.add_trace(train_trace)\n",
    "fig.add_trace(val_trace)\n",
    "fig.add_trace(test_trace)\n",
    "fig.add_trace(all_trace)\n",
    "\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(final_df.iloc[train_start:val_start])\n",
    "df_scaled = pd.DataFrame(scaler.transform(final_df))\n",
    "df_scaled.columns = final_df.columns\n",
    "print(f'We calculate the mean and std of the training data and use this to normalise the entire dataframe')\n",
    "print(final_df.head(1))\n",
    "print(df_scaled.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = len(df_scaled['cl_temp'])\n",
    "data = df_scaled['cl_temp'].to_numpy()\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(total_length - context_hours):\n",
    "    X.append(data[i:i + context_hours])\n",
    "    y.append(data[i + context_hours:i + context_hours + 1]) \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import mlflow\n",
    "# import numpy as np\n",
    "# from tensorflow import keras\n",
    "# from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, InputLayer\n",
    "\n",
    "# feature_count = len(features)\n",
    "# model = keras.Sequential()\n",
    "\n",
    "# model.add(InputLayer((context_hours, feature_count)))\n",
    "# model.add(Bidirectional(LSTM(64, activation='tanh')))\n",
    "# model.add(Dense(256, 'relu'))\n",
    "# model.add(Dense(64, 'relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(feature_count, 'linear'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.metrics import RootMeanSquaredError\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# learning_rate = 0.0005\n",
    "# epochs = 8\n",
    "# loss = 'mse'\n",
    "# batch_size = 128\n",
    "# model.compile(loss=loss, optimizer=Adam(learning_rate = learning_rate), metrics=[RootMeanSquaredError()])\n",
    "# history = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, batch_size=batch_size, verbose=False)\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scale and get mae\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred = scaler.inverse_transform(y_pred)\n",
    "y_test_final = scaler.inverse_transform(y_test)\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# print(f'MAE: {round(mean_absolute_error(y_test_final, y_pred), 2)}')\n",
    "# print(f'RMSE: {round(sqrt(mean_squared_error(y_test_final, y_pred)), 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(x=[i for i in range(len(y_pred))], y=y_test_final.reshape(len(y_test_final),) )\n",
    "# plt.scatter(x=[i for i in range(len(y_pred))], y=y_pred.reshape(len(y_pred),) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('cf_cl.h5')\n",
    "# model.save('test_model.keras')\n",
    "# # loaded_model = tf.keras.models.load_model('heath.h5')\n",
    "# loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "loaded_model = tf.keras.models.load_model('test_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.save('test_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"test_model_lite\")\n",
    "tflite_model = converter.convert()\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_temperature = X_test[0]\n",
    "true_temperature = y_test_final[0]\n",
    "predicted_temperature_scaled = loaded_model.predict(input_temperature.reshape(1, input_temperature.shape[0]))\n",
    "predicted_temperature = scaler.inverse_transform(predicted_temperature_scaled)\n",
    "\n",
    "print('True temperature: ', true_temperature)\n",
    "print('predicted_temperature: ', predicted_temperature)\n",
    "print('Input data: ', scaler.inverse_transform(input_temperature.reshape(input_temperature.shape[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
